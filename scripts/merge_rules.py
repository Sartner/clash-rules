#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Clashè§„åˆ™æ–‡ä»¶åˆå¹¶å·¥å…·
æ”¯æŒå¤šç»„URLæºï¼Œæ¯ç»„è¾“å‡ºç‹¬ç«‹çš„é…ç½®æ–‡ä»¶
"""

import yaml
import requests
import argparse
import sys
import hashlib
import os
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional
from pathlib import Path
import time


class ConfigManager:
    """é…ç½®ç®¡ç†å™¨ - è´Ÿè´£åŠ è½½å’ŒéªŒè¯YAMLé…ç½®"""

    def __init__(self, config_path: str):
        self.config_path = config_path
        self.config = {}

    def load_config(self) -> Dict:
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)

            # éªŒè¯é…ç½®
            if not self._validate_config():
                print("âŒ é…ç½®æ–‡ä»¶éªŒè¯å¤±è´¥")
                return {}

            print(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {self.config_path}")
            return self.config
        except FileNotFoundError:
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
            return {}
        except yaml.YAMLError as e:
            print(f"âŒ YAMLè§£æé”™è¯¯: {e}")
            return {}
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®å¤±è´¥: {e}")
            return {}

    def _validate_config(self) -> bool:
        """éªŒè¯é…ç½®æ ¼å¼"""
        if not self.config:
            print("âŒ é…ç½®ä¸ºç©º")
            return False

        # æ£€æŸ¥å…¨å±€é…ç½®
        if 'global' not in self.config:
            self.config['global'] = {}
            print("âš ï¸  æœªæ‰¾åˆ°å…¨å±€é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼")

        # æ£€æŸ¥è§„åˆ™ç»„
        if 'rule_groups' not in self.config:
            print("âŒ æœªæ‰¾åˆ° rule_groups é…ç½®")
            return False

        if not isinstance(self.config['rule_groups'], list):
            print("âŒ rule_groups å¿…é¡»æ˜¯åˆ—è¡¨")
            return False

        if not self.config['rule_groups']:
            print("âŒ rule_groups ä¸èƒ½ä¸ºç©º")
            return False

        # éªŒè¯æ¯ä¸ªè§„åˆ™ç»„
        for i, group in enumerate(self.config['rule_groups']):
            if not self._validate_rule_group(group, i):
                return False

        return True

    def _validate_rule_group(self, group: Dict, index: int) -> bool:
        """éªŒè¯è§„åˆ™ç»„é…ç½®"""
        if not isinstance(group, dict):
            print(f"âŒ è§„åˆ™ç»„ {index} å¿…é¡»æ˜¯å­—å…¸")
            return False

        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        if 'name' not in group:
            print(f"âŒ è§„åˆ™ç»„ {index} ç¼ºå°‘ name å­—æ®µ")
            return False

        if 'output' not in group:
            print(f"âŒ è§„åˆ™ç»„ {index} ç¼ºå°‘ output å­—æ®µ")
            return False

        if 'sources' not in group:
            print(f"âŒ è§„åˆ™ç»„ {index} ç¼ºå°‘ sources å­—æ®µ")
            return False

        if not isinstance(group['sources'], list):
            print(f"âŒ è§„åˆ™ç»„ {index} çš„ sources å¿…é¡»æ˜¯åˆ—è¡¨")
            return False

        if not group['sources']:
            print(f"âŒ è§„åˆ™ç»„ {index} çš„ sources ä¸èƒ½ä¸ºç©º")
            return False

        # éªŒè¯æ¯ä¸ªæº
        for j, source in enumerate(group['sources']):
            if not isinstance(source, dict):
                print(f"âŒ è§„åˆ™ç»„ {index} çš„æº {j} å¿…é¡»æ˜¯å­—å…¸")
                return False

            if 'url' not in source:
                print(f"âŒ è§„åˆ™ç»„ {index} çš„æº {j} ç¼ºå°‘ url å­—æ®µ")
                return False

        return True

    def get_global_config(self) -> Dict:
        """è·å–å…¨å±€é…ç½®"""
        return self.config.get('global', {})

    def get_rule_groups(self) -> List[Dict]:
        """è·å–è§„åˆ™ç»„åˆ—è¡¨"""
        return self.config.get('rule_groups', [])


class RuleSource:
    """è§„åˆ™æº - è¡¨ç¤ºä¸€ä¸ªURLæº"""

    def __init__(self, url: str, name: str = "", retries: int = 3, timeout: int = 60):
        self.url = url
        self.name = name or url
        self.retries = retries
        self.timeout = timeout
        self.data: Optional[Dict] = None
        self.raw_text: str = ""
        self.error: Optional[str] = None

    def download(self) -> bool:
        """
        ä¸‹è½½è§„åˆ™æ–‡ä»¶
        è¿”å›: æ˜¯å¦æˆåŠŸ
        """
        for attempt in range(self.retries):
            try:
                print(f"  ğŸ“¥ æ­£åœ¨ä¸‹è½½: {self.name}")
                print(f"      URL: {self.url}")
                print(f"      å°è¯• {attempt + 1}/{self.retries}")

                response = requests.get(self.url, timeout=self.timeout)
                response.raise_for_status()

                self.raw_text = response.text

                # è§£æYAML
                try:
                    self.data = yaml.safe_load(self.raw_text)
                except yaml.YAMLError as e:
                    self.error = f"YAMLè§£æå¤±è´¥: {e}"
                    print(f"  âŒ {self.error}")
                    continue

                if not self.data or 'payload' not in self.data:
                    self.error = "æ— æ•ˆçš„è§„åˆ™æ–‡ä»¶æ ¼å¼"
                    print(f"  âŒ {self.error}")
                    continue

                print(f"  âœ… ä¸‹è½½æˆåŠŸ: {len(self.data['payload'])} æ¡è§„åˆ™")
                return True

            except requests.RequestException as e:
                self.error = f"ä¸‹è½½å¤±è´¥: {e}"
                print(f"  âŒ {self.error} (å°è¯• {attempt + 1}/{self.retries})")
                if attempt < self.retries - 1:
                    print("      ç­‰å¾…2ç§’åé‡è¯•...")
                    time.sleep(2)
            except Exception as e:
                self.error = f"æœªçŸ¥é”™è¯¯: {e}"
                print(f"  âŒ {self.error}")
                break

        return False

    def extract_header(self) -> str:
        """æå–æºæ–‡ä»¶çš„æ³¨é‡Šå¤´éƒ¨"""
        if not self.raw_text:
            return ""

        lines = self.raw_text.split('\n')
        header_lines = []

        for line in lines:
            stripped = line.strip()
            if stripped.startswith('#') or stripped == '':
                header_lines.append(line)
            else:
                break

        return '\n'.join(header_lines)


class RuleGroup:
    """è§„åˆ™ç»„ - è¡¨ç¤ºä¸€ç»„è¦åˆå¹¶çš„æº"""

    def __init__(self, config: Dict, global_config: Dict):
        self.name = config['name']
        self.description = config.get('description', '')
        self.output_file = config['output']
        self.output_dir = global_config.get('output_dir', '')
        self.custom_header = config.get('header', [])
        self.deduplication = config.get('deduplication', global_config.get('deduplication', 'group'))
        self.sources = [
            RuleSource(
                source['url'],
                source.get('name', source['url']),
                global_config.get('retries', 3),
                global_config.get('timeout', 60)
            )
            for source in config['sources']
        ]
        self.stats = {
            'total_sources': len(self.sources),
            'successful_sources': 0,
            'failed_sources': 0,
            'total_rules': 0,
            'deduplicated_rules': 0,
            'removed_count': 0
        }

    def add_source(self, source: RuleSource):
        """æ·»åŠ è§„åˆ™æº"""
        self.sources.append(source)

    def merge(self) -> Tuple[Dict, str]:
        """
        åˆå¹¶è§„åˆ™ç»„ä¸­çš„æ‰€æœ‰æº
        è¿”å›: (åˆå¹¶åçš„æ•°æ®, å¤´éƒ¨æ³¨é‡Š)
        """
        print(f"\nğŸ“¦ å¼€å§‹å¤„ç†è§„åˆ™ç»„: {self.name}")
        print(f"   è¾“å‡ºæ–‡ä»¶: {self.output_file}")
        print(f"   æºæ•°é‡: {len(self.sources)}")
        print(f"   å»é‡ç­–ç•¥: {self.deduplication}")

        # ä¸‹è½½æ‰€æœ‰æº
        self._download_all_sources()

        # æå–æ•°æ®
        all_payloads = []
        for source in self.sources:
            if source.data and 'payload' in source.data:
                all_payloads.append((source, source.data['payload']))

        if not all_payloads:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è§„åˆ™æ•°æ®")
            return {"payload": [], "version": 1}, ""

        # åˆå¹¶å’Œå»é‡
        merged_data, dedup_stats = self._merge_and_deduplicate(all_payloads)

        # è®¡ç®—Payloadçš„MD5å€¼
        payload_md5 = self._calculate_payload_md5(merged_data['payload'])

        # ç”Ÿæˆå¤´éƒ¨
        header = self._generate_header(dedup_stats, payload_md5)

        return merged_data, header

    def _download_all_sources(self):
        """ä¸‹è½½æ‰€æœ‰æº"""
        print("\n  ğŸ“¥ æ­£åœ¨ä¸‹è½½æº...")

        for source in self.sources:
            if source.download():
                self.stats['successful_sources'] += 1
            else:
                self.stats['failed_sources'] += 1
                print(f"  âš ï¸  æºä¸‹è½½å¤±è´¥: {source.name}")

    def _merge_and_deduplicate(self, sources_data: List[Tuple[RuleSource, List]]) -> Tuple[Dict, Dict]:
        """åˆå¹¶å’Œå»é‡"""
        if self.deduplication == 'none':
            # ä¸å»é‡
            all_rules = []
            for source, payload in sources_data:
                all_rules.append({
                    'name': source.name,
                    'rules': payload
                })

            total = sum(len(payload) for _, payload in sources_data)
            self.stats['total_rules'] = total
            self.stats['deduplicated_rules'] = total
            self.stats['removed_count'] = 0

            return {
                "payload": all_rules,
                "version": 1
            }, {'total': total, 'deduplicated': total, 'removed': 0}

        elif self.deduplication == 'group':
            # ç»„å†…å»é‡
            print("\n  ğŸ”„ æ‰§è¡Œç»„å†…å»é‡...")

            seen = set()
            unique_rules = []
            total = 0

            for source, payload in sources_data:
                for rule in payload:
                    total += 1
                    rule_str = str(rule).strip()
                    if rule_str and rule_str not in seen:
                        seen.add(rule_str)
                        unique_rules.append(rule)

            self.stats['total_rules'] = total
            self.stats['deduplicated_rules'] = len(unique_rules)
            self.stats['removed_count'] = total - len(unique_rules)

            return {
                "payload": unique_rules,
                "version": 1
            }, {'total': total, 'deduplicated': len(unique_rules), 'removed': total - len(unique_rules)}

        elif self.deduplication == 'all':
            # å…¨å±€å»é‡ï¼ˆæ‰©å±•ç”¨ï¼Œå½“å‰ç­‰ä»·äºç»„å†…ï¼‰
            return self._merge_and_deduplicate(sources_data)

        else:
            print(f"  âš ï¸  æœªçŸ¥å»é‡ç­–ç•¥: {self.deduplication}ï¼Œä½¿ç”¨ç»„å†…å»é‡")
            return self._merge_and_deduplicate(sources_data)

    def _generate_header(self, dedup_stats: Dict, payload_md5: str = "") -> str:
        """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å¤´éƒ¨"""
        lines = []

        # è‡ªå®šä¹‰å¤´éƒ¨
        if self.custom_header:
            lines.extend(self.custom_header)
        else:
            # é»˜è®¤å¤´éƒ¨
            lines.append(f"# NAME: {self.name} (Merged)")
            if self.description:
                lines.append(f"# DESCRIPTION: {self.description}")

            lines.append("# AUTHOR: blackmatrix7 (merged by script)")
            lines.append("# REPO: https://github.com/blackmatrix7/ios_rule_script")

        # æ—¶é—´æˆ³
        lines.append(f"# UPDATED: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # MD5æ ¡éªŒå€¼
        if payload_md5:
            lines.append(f"# MD5 (Payload): {payload_md5}")

        # æºä¿¡æ¯
        lines.append("")
        lines.append("# SOURCES:")
        for source in self.sources:
            if source.data:
                lines.append(f"#   - {source.name}: {source.url}")

        # ç»Ÿè®¡ä¿¡æ¯
        lines.append("")
        lines.append("# STATS:")
        lines.append(f"#   Total Sources: {self.stats['total_sources']}")
        lines.append(f"#   Successful: {self.stats['successful_sources']}")
        lines.append(f"#   Failed: {self.stats['failed_sources']}")
        lines.append(f"#   Total Rules: {dedup_stats['total']}")
        lines.append(f"#   Deduplicated: {dedup_stats['deduplicated']}")
        lines.append(f"#   Removed: {dedup_stats['removed']}")
        lines.append("")
        lines.append("---")
        lines.append("")

        return '\n'.join(lines)

    def _calculate_payload_md5(self, payload: List) -> str:
        """
        è®¡ç®—Payloadåˆ—è¡¨çš„MD5å€¼
        """
        try:
            # å°†payloadåºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²
            payload_lines = []

            # æ£€æŸ¥æ•°æ®æ ¼å¼
            if payload and len(payload) > 0 and isinstance(payload[0], dict) and 'name' in payload[0]:
                # åˆ†ç»„æ ¼å¼ï¼šå…ˆæŒ‰ç»„åæ’åºï¼Œå†æŒ‰è§„åˆ™æ’åº
                for group in sorted(payload, key=lambda x: x.get('name', '')):
                    for rule in sorted(group.get('rules', []), key=str):
                        payload_lines.append(str(rule).strip())
            else:
                # å¹³é¢æ ¼å¼ï¼šç›´æ¥æ’åº
                for rule in sorted(payload, key=str):
                    payload_lines.append(str(rule).strip())

            # è¿‡æ»¤ç©ºè¡Œ
            payload_lines = [line for line in payload_lines if line]

            # è®¡ç®—MD5
            payload_text = '\n'.join(payload_lines)
            return hashlib.md5(payload_text.encode('utf-8')).hexdigest()
        except Exception as e:
            print(f"  âš ï¸  MD5è®¡ç®—å¤±è´¥: {e}")
            return ""

    def save(self, data: Dict, header: str) -> bool:
        """ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶"""
        try:
            # æ„å»ºå®Œæ•´çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
            if self.output_dir:
                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                os.makedirs(self.output_dir, exist_ok=True)
                # ç»„åˆå®Œæ•´è·¯å¾„
                output_path = os.path.join(self.output_dir, os.path.basename(self.output_file))
            else:
                output_path = self.output_file

            # å¦‚æœè¾“å‡ºæ–‡ä»¶åŒ…å«ç›®å½•è·¯å¾„ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨
            output_dir = os.path.dirname(output_path)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)

            print(f"\n  ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°: {output_path}")

            with open(output_path, 'w', encoding='utf-8') as f:
                # å†™å…¥å¤´éƒ¨
                f.write(header)

                # æ£€æŸ¥æ•°æ®æ ¼å¼
                if data['payload'] and isinstance(data['payload'][0], dict) and 'name' in data['payload'][0]:
                    # æ ¼å¼1: åˆ†ç»„æ ¼å¼
                    for group in data['payload']:
                        f.write(f"\n# === {group['name']} ===\n")
                        for rule in group['rules']:
                            f.write(f"- {rule}\n")
                else:
                    # æ ¼å¼2: å¹³é¢æ ¼å¼
                    f.write("payload:\n")
                    for rule in data['payload']:
                        f.write(f"- {rule}\n")

                # ç‰ˆæœ¬ä¿¡æ¯
                f.write(f"\nversion: {data['version']}\n")

            print(f"  âœ… ä¿å­˜æˆåŠŸ")
            return True

        except Exception as e:
            print(f"  âŒ ä¿å­˜å¤±è´¥: {e}")
            return False


class RuleMerger:
    """è§„åˆ™åˆå¹¶å™¨ - ä¸»æ§åˆ¶å™¨"""

    def __init__(self, config_path: Optional[str] = None, output_dir: str = ''):
        self.config_path = config_path
        self.config_manager: Optional[ConfigManager] = None
        self.output_dir = output_dir

    async def process_all_groups(self):
        """å¤„ç†æ‰€æœ‰è§„åˆ™ç»„"""
        if self.config_path:
            # ä½¿ç”¨é…ç½®æ–‡ä»¶
            self.config_manager = ConfigManager(self.config_path)
            config = self.config_manager.load_config()

            if not config:
                print("âŒ åŠ è½½é…ç½®å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
                return 1

            # åˆå¹¶è¾“å‡ºç›®å½•ï¼šå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆ
            global_config = self.config_manager.get_global_config()
            if self.output_dir:
                global_config['output_dir'] = self.output_dir

            rule_groups = self.config_manager.get_rule_groups()

            print(f"\n{'=' * 60}")
            print(f"ğŸ“‹ å…±æ‰¾åˆ° {len(rule_groups)} ä¸ªè§„åˆ™ç»„")
            print(f"{'=' * 60}")

            success_count = 0
            fail_count = 0

            for i, group_config in enumerate(rule_groups, 1):
                print(f"\n{'=' * 60}")
                print(f"ğŸ”„ å¤„ç† [{i}/{len(rule_groups)}] {group_config['name']}")
                print(f"{'=' * 60}")

                try:
                    group = RuleGroup(group_config, global_config)
                    merged_data, header = group.merge()

                    if group.save(merged_data, header):
                        success_count += 1
                        print(f"\nâœ… è§„åˆ™ç»„ [{i}/{len(rule_groups)}] å¤„ç†å®Œæˆ")
                    else:
                        fail_count += 1
                        print(f"\nâŒ è§„åˆ™ç»„ [{i}/{len(rule_groups)}] å¤„ç†å¤±è´¥")

                except Exception as e:
                    fail_count += 1
                    print(f"\nâŒ è§„åˆ™ç»„ [{i}/{len(rule_groups)}] å‡ºç°å¼‚å¸¸: {e}")

            print(f"\n{'=' * 60}")
            print(f"ğŸ“Š å¤„ç†å®Œæˆ!")
            print(f"   æˆåŠŸ: {success_count} ä¸ª")
            print(f"   å¤±è´¥: {fail_count} ä¸ª")
            print(f"{'=' * 60}")

        else:
            # å…¼å®¹æ¨¡å¼ï¼šä½¿ç”¨é»˜è®¤é…ç½®
            print("âš ï¸  æœªæŒ‡å®šé…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨å…¼å®¹æ¨¡å¼")
            return self._run_compat_mode()

        return 0

    def _run_compat_mode(self) -> int:
        """å…¼å®¹æ¨¡å¼ï¼šä½¿ç”¨åŸå§‹è„šæœ¬çš„é…ç½®"""
        print("\n" + "=" * 60)
        print("ğŸ”„ å…¼å®¹æ¨¡å¼ - ä½¿ç”¨é»˜è®¤é…ç½®")
        print("=" * 60)

        # åŸå§‹URL
        OPENAI_URL = "https://gh-proxy.com/raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Clash/OpenAI/OpenAI.yaml"
        GEMINI_URL = "https://gh-proxy.com/raw.githubusercontent.com/blackmatrix7/ios_rule_script/refs/heads/master/rule/Clash/Gemini/Gemini.yaml"
        OUTPUT_FILE = "sartner-ai.yaml"

        # ä½¿ç”¨åŸå§‹æµç¨‹
        source1 = RuleSource(OPENAI_URL, "OpenAI")
        source2 = RuleSource(GEMINI_URL, "Gemini")

        print("\nğŸ“¥ æ­£åœ¨ä¸‹è½½æº...")
        if not source1.download():
            print("âŒ OpenAI ä¸‹è½½å¤±è´¥")
            return 1
        if not source2.download():
            print("âŒ Gemini ä¸‹è½½å¤±è´¥")
            return 1

        # åˆå¹¶
        print("\nğŸ”„ æ­£åœ¨åˆå¹¶è§„åˆ™...")
        seen = set()
        unique_rules = []
        total = 0

        for source in [source1, source2]:
            if source.data and 'payload' in source.data:
                for rule in source.data['payload']:
                    total += 1
                    rule_str = str(rule).strip()
                    if rule_str and rule_str not in seen:
                        seen.add(rule_str)
                        unique_rules.append(rule)

        # å¤´éƒ¨
        header_lines = [
            "# NAME: OpenAI + Gemini (Merged)",
            "# AUTHOR: blackmatrix7 (merged by script)",
            f"# UPDATED: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"# TOTAL: {len(unique_rules)}",
            "",
            ""
        ]
        header = '\n'.join(header_lines)

        # ä¿å­˜
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°: {OUTPUT_FILE}")
        try:
            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                f.write(header)
                f.write("payload:\n")

                for rule in unique_rules:
                    f.write(f"- {rule}\n")

                f.write(f"\nversion: 1\n")

            print(f"âœ… åˆå¹¶å®Œæˆ! è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
            print(f"   è§„åˆ™æ€»æ•°: {len(unique_rules)}")
            return 0
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return 1


def create_quick_config(args) -> Optional[str]:
    """åˆ›å»ºå¿«é€Ÿé…ç½®çš„ä¸´æ—¶é…ç½®æ–‡ä»¶"""
    if not args.url or not args.output:
        return None

    config = {
        'global': {
            'deduplication': 'group',
            'retries': 3,
            'timeout': 60
        },
        'rule_groups': [{
            'name': 'quick-group',
            'description': f'Quick merge of {len(args.url)} sources',
            'output': args.output,
            'sources': [{'url': url, 'name': url} for url in args.url]
        }]
    }

    temp_config = 'temp_quick_config.yaml'
    with open(temp_config, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)

    return temp_config


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Clashè§„åˆ™æ–‡ä»¶åˆå¹¶å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨é…ç½®æ–‡ä»¶
  python merge_rules.py --config config.yaml

  # ä½¿ç”¨é…ç½®æ–‡ä»¶å¹¶æŒ‡å®šè¾“å‡ºç›®å½•
  python merge_rules.py --config config.yaml --output-dir ./output

  # å¿«é€Ÿåˆå¹¶ä¸¤ä¸ªURL
  python merge_rules.py --url url1.yaml --url url2.yaml --output merged.yaml

  # å¿«é€Ÿåˆå¹¶å¹¶æŒ‡å®šè¾“å‡ºç›®å½•
  python merge_rules.py --url url1.yaml --url url2.yaml --output merged.yaml --output-dir ./output

  # å…¼å®¹æ¨¡å¼ï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
  python merge_rules.py
        """
    )

    # ä¸»é…ç½®
    parser.add_argument(
        '--config', '-c',
        type=str,
        help='YAMLé…ç½®æ–‡ä»¶è·¯å¾„'
    )

    # å¿«é€Ÿé…ç½®
    parser.add_argument(
        '--url', '-u',
        action='append',
        help='è§„åˆ™æ–‡ä»¶URLï¼ˆå¯æŒ‡å®šå¤šæ¬¡ï¼‰'
    )

    parser.add_argument(
        '--output', '-o',
        type=str,
        help='è¾“å‡ºæ–‡ä»¶åï¼ˆé…åˆ--urlä½¿ç”¨ï¼‰'
    )

    parser.add_argument(
        '--output-dir',
        type=str,
        default='',
        help='è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œæ‰€æœ‰æ–‡ä»¶å°†ä¿å­˜åˆ°æ­¤ç›®å½•ï¼‰'
    )

    # å…¶ä»–é€‰é¡¹
    parser.add_argument(
        '--dedup', '-d',
        type=str,
        choices=['group', 'all', 'none'],
        default='group',
        help='å»é‡ç­–ç•¥: group=ç»„å†…, all=å…¨å±€, none=ä¸å»é‡'
    )

    args = parser.parse_args()

    # åˆ‡æ¢åˆ°è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œç¡®ä¿è·¯å¾„æ­£ç¡®è§£æ
    script_dir = os.path.dirname(os.path.abspath(__file__))
    os.chdir(script_dir)
    print(f"ğŸ“‚ å·¥ä½œç›®å½•: {os.getcwd()}")

    # éªŒè¯è¾“å‡ºç›®å½•
    if args.output_dir:
        # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        args.output_dir = os.path.abspath(args.output_dir)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")

    # å¦‚æœé…ç½®æ–‡ä»¶è·¯å¾„ä»¥ scripts/ å¼€å¤´ï¼Œç§»é™¤å‰ç¼€
    if args.config and args.config.startswith('scripts/'):
        args.config = args.config[8:]  # ç§»é™¤ 'scripts/' å‰ç¼€
        print(f"ğŸ“„ é…ç½®æ–‡ä»¶: {args.config}")

    # æ£€æŸ¥å‚æ•°
    if args.config and (args.url or args.output):
        print("âŒ ä¸èƒ½åŒæ—¶ä½¿ç”¨ --config å’Œ --url/--output")
        return 1

    if (args.url and not args.output) or (args.output and not args.url):
        print("âŒ ä½¿ç”¨ --url æ—¶å¿…é¡»æŒ‡å®š --output")
        return 1

    print("\n" + "=" * 60)
    print("ğŸš€ Clashè§„åˆ™åˆå¹¶å™¨")
    print("=" * 60)

    merger = RuleMerger()

    # å¤„ç†é…ç½®
    if args.config:
        # ä½¿ç”¨é…ç½®æ–‡ä»¶
        merger = RuleMerger(config_path=args.config, output_dir=args.output_dir)
        import asyncio
        return asyncio.run(merger.process_all_groups())

    elif args.url:
        # å¿«é€Ÿé…ç½®
        temp_config = create_quick_config(args)
        if temp_config:
            merger = RuleMerger(config_path=temp_config, output_dir=args.output_dir)
            import asyncio
            result = asyncio.run(merger.process_all_groups())

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.remove(temp_config)
            except:
                pass

            return result
        return 1

    else:
        # å…¼å®¹æ¨¡å¼
        return merger._run_compat_mode()


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
